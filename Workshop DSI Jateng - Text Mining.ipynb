{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materi Workshop Text Mining \n",
    "*19 Januari 2019 | DSI Jateng*\n",
    "***\n",
    "\n",
    "# Apa itu Text Mining?\n",
    "Text Mining adalah sebuah ilmu yang membantu kita membersihkan data yang tidak terstruktur. Data yang telah bersih kemudian dilakukan analisis dengan ilmu yang disebut dengan Text Analysis. Berikut ini adalah beberapa proses analisis yang terdapat pada Text Analysis:\n",
    " \n",
    "1. Entity Extraction\n",
    "2. Keyword Extraction\n",
    "3. Semantic Analysis\n",
    "4. Sentiment Analysis\n",
    "5. dll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entity Extraction\n",
    "\n",
    "Entitity Extraction adalah proses melakukan klasifikasi element dari sebuah teks pada objek tertentu. Misalkan melakukan ekstraksi pada objek nama orang, nama sebuah organisasi, tempat, dll. Berikut ini adalah contohnya dari teks Wikipedia dengan sedikit perubahan:\n",
    "\n",
    "> Indonesia adalah negara di Asia Tenggara yang dilintasi garis khatulistiwa dan berada di antara daratan benua Asia dan Australia, serta antara Samudra Pasifik dan Samudra Hindia. Indonesia merupakan anggota dari PBB dan satu-satunya anggota yang pernah keluar dari PBB, yaitu pada tanggal 7 Januari 1965, dan bergabung kembali pada tanggal 28 September 1966 dan Indonesia tetap dinyatakan sebagai anggota yang ke-60, keanggotaan yang sama sejak bergabungnya Indonesia pada tanggal 28 September 1950. Presiden dan Wakil Presiden pada periode 2014-2019 adalah Bapak Joko Widodo dan Bapak Jusuf Kalla.\n",
    "\n",
    "Berdasarkan teks tersebut, setelah dilakukan Entity Extraction didapatkan Entities sebagai berikut:\n",
    "- Indonesia\n",
    "- Australia\n",
    "- PBB\n",
    "- Joko Widodo\n",
    "- Jusuf Kalla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keyword Extraction\n",
    "\n",
    "Keyword Extraction adalah proses mendapatkan *keyword*/kata kunci dari sebuah teks. Dengan mendapatkan *keyword* dari sebuah teks, kita bisa melakukan *indexing data*, menghasilkan *tag clouds*, dan mempercepat proses pencarian. Berikut adalah contohnya:\n",
    "\n",
    "> Indonesia adalah negara di Asia Tenggara yang dilintasi garis khatulistiwa dan berada di antara daratan benua Asia dan Australia, serta antara Samudra Pasifik dan Samudra Hindia. Indonesia merupakan anggota dari PBB dan satu-satunya anggota yang pernah keluar dari PBB, yaitu pada tanggal 7 Januari 1965, dan bergabung kembali pada tanggal 28 September 1966 dan Indonesia tetap dinyatakan sebagai anggota yang ke-60, keanggotaan yang sama sejak bergabungnya Indonesia pada tanggal 28 September 1950. Presiden dan Wakil Presiden pada periode 2014-2019 adalah Bapak Joko Widodo dan Bapak Jusuf Kalla.\n",
    "\n",
    "Bisa didapatkan *keyword*, sebagai berikut:\n",
    "- garis khatulistiwa\n",
    "- anggota PBB\n",
    "- presiden\n",
    "- wakil presiden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Analysis\n",
    "\n",
    "Semantic Analysis adalah proses membuat mesin untuk belajar makna dari sebuah kata/kalimat, serta memahami keterkaitan hubungan dengan kata/kalimat lainnya. Hal ini bisa dilakukan dengan memperhatikan data dari frekuensi, *proximity* (kedekatan), dan cara lainnya. Sebagai contoh adalah sebagai berikut:\n",
    "- \"Anggora\" dan \"kucing\" memiliki keterkaitan hubungan secara semantik.\n",
    "- \"Anggora\" dan \"lebat\" lebih memiliki kedekatan dibandingkan \"kucing\" dan \"lebat\".\n",
    "- \"Anggora\" huruf depannya lebih sering kapital dibanding kata obyek lain.\n",
    "- \"lebat\" bisa juga berarti \"rimbun\" atau \"deras\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Sentiment Analysis (Yang akan kita lakukan!)\n",
    "\n",
    "Sentiment Analysis adalah proses melakukan identifikasi dan ekstrak informasi dari data teks untuk mengetahui sentimen dari produk yang bisa berupa positif, negatif, atau netral. Berikut ini adalah contohnya dari review film **Spider-Man: Into the Spider-Verse**:\n",
    "* positif: \"Keren banget animasinya, film animasi terbaik yang pernah saya tonton!\"\n",
    "* negatif: \"Biasa aja gak ada seru-serunya, alur ceritanya jelek.\"\n",
    "* netral: \"Pertamax gan!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengenalan Python\n",
    "\n",
    "Python memiliki syntax bahasa yang sederhana dan mudah dipahami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data integer\n",
    "data_integer = 5\n",
    "print(type(data_integer))\n",
    "\n",
    "# data float\n",
    "# To do: definisikan nilai variabel data_float dengan tipe data float\n",
    "data_float = None\n",
    "print(type(data_float))\n",
    "\n",
    "# data string\n",
    "# To do: definisikan nilai varibel data_string dengan tipe data string\n",
    "data_string = None\n",
    "print(type(data_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operasi dasar matematika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 3\n",
    "\n",
    "# pertambahan, pengurangan, pembagian, perkalian, modulo, division\n",
    "# TODO: isilah operasi dasar matematika untuk a dan b\n",
    "tambah = None\n",
    "kurang = None\n",
    "bagi = None\n",
    "kali = None\n",
    "mod = None\n",
    "div = None\n",
    "\n",
    "print(\"Hasil a + b:\", tambah)\n",
    "print(\"Hasil a - b:\", kurang)\n",
    "print(\"Hasil a / b:\", bagi)\n",
    "print(\"Hasil a * b:\", kali)\n",
    "print(\"Hasil a mod b:\", mod)\n",
    "print(\"Hasil a div b:\", div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operasi dasar string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks1 = \"Ibu pergi ke pasar\"\n",
    "teks2 = \"Budi bermain bola bersama teman\"\n",
    "\n",
    "# concatenation\n",
    "# TODO: lakukan concatenation pada teks1 dan teks2\n",
    "concat = None\n",
    "print(\"Hasil concatenation:\", concat)\n",
    "\n",
    "# split teks\n",
    "# TODO: lakukan operasi split teks berdasarkan spasi\n",
    "split = None\n",
    "print(\"Hasil split teks:\", split)\n",
    "\n",
    "# uppercase dan lowercase\n",
    "# TODO: ubah teks1 menjadi upper case dan teks2 menjadi lower case\n",
    "upper = None\n",
    "lower = None\n",
    "print(\"Hasil uppercase:\", upper)\n",
    "print(\"Hasil lowercase:\", lower)\n",
    "\n",
    "# substring\n",
    "# TODO: cetak kata 'Ibu' pada teks1 dan cetak kata 'bola' pada teks2\n",
    "ibu = None\n",
    "bola = None\n",
    "print(\"Substring kata:\", ibu)\n",
    "print(\"Substring kata:\", bola)\n",
    "\n",
    "# count\n",
    "# TODO: hitung kemunculan 'ber' pada teks2\n",
    "count_ber = None\n",
    "print(\"Kemunculan 'ber' sebanyak:\", count_ber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List, Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list\n",
    "list_buah = ['apel', 'mangga', 'pisang']\n",
    "\n",
    "# TODO: cetak panjang list buah\n",
    "panjang_list = None\n",
    "print(\"Panjang list buah adalah:\", panjang_list)\n",
    "\n",
    "# TODO: cetak elemen 'mangga' dari list buah\n",
    "mangga = None\n",
    "print(\"Buah :\", mangga)\n",
    "\n",
    "# TODO: cetak elemen 'apel' dan 'pisang' dari list buah\n",
    "mangga = None\n",
    "print(\"Buah :\", mangga)\n",
    "\n",
    "# TODO: tambah data 'jeruk' pada list buah\n",
    "list_buah = None\n",
    "print(\"List buah setelah ditambah jeruk:\", list_buah)\n",
    "\n",
    "# TODO: hapus data 'jeruk' pada list buah\n",
    "list_buah = None\n",
    "print(\"List buah setelah dihapus jeruk:\", list_buah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary\n",
    "dict_objek = {'mobil': 'honda', \n",
    "              'buah': ['apel', 'mangga', 'pisang']}\n",
    "\n",
    "# TODO: cetak key yang terdapat pada dictionary objek\n",
    "keys = None\n",
    "print(\"Key dari dictionary objek:\", keys)\n",
    "\n",
    "# TODO: cetak value yang terdapat pada dictionary objek\n",
    "values = None\n",
    "print(\"Values dari dictionary objek:\", values)\n",
    "\n",
    "# TODO: cetak value dari key 'buah'\n",
    "value_buah = None\n",
    "print(\"Value dari key 'buah' adalah:\", value_buah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengulangan dengan 'for'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hewan = ['kucing', 'penguin', 'burung']\n",
    "\n",
    "# TODO: lakukan cetak semua elemen pada list hewan\n",
    "\n",
    "\n",
    "# TODO: buat sebuah list yang berisi hewan berakhiran huruf 'g'dengan perulangan hanya satu baris\n",
    "list_hewan_akhiran_g = None\n",
    "print(\"List hewan berakhiran huruf 'g':\", list_hewan_akhiran_g)\n",
    "\n",
    "# TODO: buat sebuah perulangan yang mencetak angka 1-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kondisi if-else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angka = 20\n",
    "\n",
    "# TODO: cetak 'genap' jika angka genap, dan cetak ganjil jika angka ganjil\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "b = 12\n",
    "\n",
    "# TODO: buatlah sebuah fungsi untuk penjumlahan\n",
    "def jumlah(a, b):\n",
    "    return\n",
    "\n",
    "print(\"Penjumlahan a + b =\", jumlah(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploring the data!\n",
    "\n",
    "Dataset yang digunakan adalah [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/). Dataset tersebut berisi review film dari website [imdb.com](imdb.com), yang setiap datanya sudah terdapat label 'positive' atau 'negative'.\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2011.\n",
    "\n",
    "Dataset sudah tersedia. Lakukan load data dengan code Python di bawah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='data/imdb-reviews'):\n",
    "    \"\"\"Read IMDb movie reviews from given directory.\n",
    "    \n",
    "    Directory structure expected:\n",
    "    - data/imdb-reviews\n",
    "        - train/\n",
    "            - pos/\n",
    "            - neg/\n",
    "        - test/\n",
    "            - pos/\n",
    "            - neg/\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Data, labels to be returned in nested dicts matching the dir. structure\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    # Assume 2 sub-directories: train, test\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "\n",
    "        # Assume 2 sub-directories for sentiment (label): pos, neg\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            # Fetch list of files for this sentiment\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            # Read reviews data and assign labels\n",
    "            for f in files:\n",
    "                with open(f, 'rb') as review:\n",
    "                    data[data_type][sentiment].append(review.read().decode('utf-8'))\n",
    "                    labels[data_type][sentiment].append(sentiment)\n",
    "            \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "    \n",
    "    # Return data, labels as nested dicts\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "data, labels = read_imdb_data()\n",
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "        len(data['train']['pos']), len(data['train']['neg']),\n",
    "        len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba kita lihat positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['train']['pos'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dan negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['train']['neg'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga bisa membuat visualisasi dengan Wordcloud dari data review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing wordcloud\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "sentiment = 'pos'\n",
    "\n",
    "# Combine all reviews for the desired sentiment\n",
    "combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
    "\n",
    "# Initialize wordcloud object\n",
    "wc = WordCloud(background_color='white', max_words=50,\n",
    "        # update stopwords to include common words like film and movie\n",
    "        stopwords = STOPWORDS.update(['br','film','movie']))\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "plt.imshow(wc.generate(combined_text))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba rubah sentiment menjadi 'neg' dan lihat perbedaannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = 'neg'\n",
    "\n",
    "combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
    "\n",
    "wc = WordCloud(background_color = 'white', max_words=50, stopwords = STOPWORDS.update(['br', 'film', 'movie']))\n",
    "plt.imshow(wc.generate(combined_text))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Form Training and test sets\n",
    "\n",
    "Setelah kita melihat bentuk data review seperti apa, selanjutnya bentuklah training set dan test set dari dokumen review positive dan negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    # TODO: Combine positive and negative reviews and labels\n",
    "    data_train = None\n",
    "    labels_train = None\n",
    "    \n",
    "    data_test = None\n",
    "    labels_test = None\n",
    "    \n",
    "    # TODO: Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = None\n",
    "    data_test, labels_test = None\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = prepare_imdb_data(data)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(data_train), len(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing\n",
    "\n",
    "Jika kita melihat contoh review, data yang kita miliki masih terdapat tag HTML, oleh karena itu perlu <font color=blue>melakukan remove tag HTML</font>. Kita juga perlu <font color=blue>menghapus karakter bukan huruf, melakukan normalisasi huruf dari uppercase ke lowercase, melakukan tokenisasi, menghapus stop words, dan melakukan stemming pada setiap kata pada dokumen.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Convert each review to words\n",
    "\n",
    "Tugas selanjutnya lengkapi fungsi <code>review_to_words()</code> yang menjalankan semua proses *preprocessing*. Pastikan sudah berhasil dilakukan *import* untuk *library* yang akan digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup to easily remove HTML tags\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# RegEx for removing non-letter characters\n",
    "import re\n",
    "\n",
    "# NLTK library for the remaining steps\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")   # download list of stopwords (only once; need not run it again)\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
    "    \n",
    "    # TODO: Remove HTML tags and non-letters,\n",
    "    soup = None\n",
    "    text = None\n",
    "    \n",
    "    # TODO: convert to lowercase, tokenize,\n",
    "    text = None\n",
    "    words = None\n",
    "    \n",
    "    # TODO: remove stopwords and stem\n",
    "    words = None\n",
    "    words = None\n",
    "\n",
    "    # Return final list of words\n",
    "    return words\n",
    "\n",
    "\n",
    "review_to_words(\"\"\"This is just a <em>test</em>.<br/><br />\n",
    "But if it wasn't a test, it would make for a <b>Great</b> movie review!\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ketika fungsi <code>review_to_words()</code> sudah disempurnakan, kita bisa menerapkannya pada data training dan data test. Prosesnya butuh waktu yang cukup lama, jadi kita membuat *cache file* agar bisa di*retrieve* nanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = list(map(review_to_words, data_train))\n",
    "        words_test = list(map(review_to_words, data_test))\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "words_train, words_test, labels_train, labels_test = preprocess_data(\n",
    "        data_train, data_test, labels_train, labels_test)\n",
    "\n",
    "# Take a look at a sample\n",
    "print(\"\\n--- Raw review ---\")\n",
    "print(data_train[1])\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[1])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extracting Bag-of-Words features\n",
    "\n",
    "Setelah document review telah dilakukan *preprocessing*, kita bisa melakukan transformasi menjadi representasi fitur Bag-of-Words. Kita hanya akan melakukannya hanya pada data training, karena kita tidak boleh melihat data testing sama sekali!\n",
    "\n",
    "**Vocabulary V** (kumpulan kata unik dokumen dari data training) akan digunakan saat melakukan training dengan algoritma *supervised learning*. Setiap data test harus dilakukan transform dengan cara yang sama untuk melakukan prediksi. Jadi, mengingat pentingnya data hasil *transform* dan *vocabulary*, maka akan kita simpan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Compute Bag-of-Words features\n",
    "\n",
    "Lengkapi fungsi <code>extract_BoW_features()</code>, kemudian terapkan pada data training dan data testing, dan simpan hasilnya pada variable <code>features_train</code> dan <code>features_test</code>. Tentukan ukuran vocabulary, misalkan $|V| = 5000$, maka yang digunakan adalah top $|V|$ dan sisanya diabaikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # TODO: Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = None\n",
    "        features_train = None\n",
    "\n",
    "        # TODO: Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = None\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary\n",
    "\n",
    "\n",
    "# Extract Bag of Words features for both training and test datasets\n",
    "features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test)\n",
    "\n",
    "# Inspect the vocabulary that was computed\n",
    "print(\"Vocabulary: {} words\".format(len(vocabulary)))\n",
    "\n",
    "import random\n",
    "print(\"Sample words: {}\".format(random.sample(list(vocabulary.keys()), 8)))\n",
    "\n",
    "# Sample\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[5])\n",
    "print(\"\\n--- Bag-of-Words features ---\")\n",
    "print(features_train[5])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan visualisasi fitur Bag-of-Words pada salah satu document training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the BoW feature vector for a training document\n",
    "plt.plot(features_train[5,:])\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Normalize feature vectors\n",
    "\n",
    "Fitur Bag-of-Words berisi jumlah pada masing-masing kata. Tetapi, jumlah pada masing-masing kata bisa bervariasi, dan memiliki potensi menurunkan performa *learning algorithm*. Jadi, kita lakukan normalisasi fitur BoW agar memiliki satuan panjang.\n",
    "\n",
    "Hal ini untuk menjaga representasi dari dokumen dan juga mencegah dokumen yang memiliki jumlah kata besar mendominasi kata  yang berjumlah sedikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pr\n",
    "\n",
    "# TODO: Normalize BoW features in training and test set\n",
    "features_train = None\n",
    "features_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classification using BoW features\n",
    "\n",
    "Setelah data dilakukan transformasi, kita bisa memasukkan ke dalam *classifier*. Kita menggunakan Naive Bayes classifier dari library scikit-learn, dan kemudian kita lakukan evaluasi pada data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# TODO: Train a Guassian Naive Bayes classifier\n",
    "clf = None\n",
    "clf.fit(None)\n",
    "\n",
    "# Calculate the mean accuracy score on training and test sets\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf.__class__.__name__,\n",
    "        clf.score(features_train, labels_train),\n",
    "        clf.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Testing/Pengujian\n",
    "\n",
    "Buatlah sebuah review movie pendek, kemudian coba keluarkan hasilnya, apakah positif atau negatif?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a sample review and set its true sentiment\n",
    "my_review = \"Despite a compelling lead performance by Tom Hanks and a great soundtrack, Forrest Gump never gets out of the shadow of its weak plot and questionable premise\"\n",
    "true_sentiment = 'neg'  # sentiment must be 'pos' or 'neg'\n",
    "\n",
    "# TODO: Apply the same preprocessing and vectorizing steps as you did for your training data\n",
    "my_review_words = None\n",
    "\n",
    "# TODO: Then call your classifier to label it\n",
    "vectorizer = None\n",
    "vectorizer = None\n",
    "my_review_features = None\n",
    "my_review_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(my_review_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
